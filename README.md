# SDLC Agent Pipeline Tool

An AI-powered SDLC automation tool that orchestrates 5 specialized agents (powered by Google Gemini) to automate workflows from requirements to release, with full audit trail capabilities.

## The 5 Agents

| Agent | Codename | Model | Description |
|-------|----------|-------|-------------|
| 1 | **SCRIBE** | gemini-2.0-flash | Transforms requirements into feature documents (Feature Doc, DPIA, Data Flow) |
| 2 | **ARCHITECT** | gemini-2.0-pro | Analyzes repositories and creates implementation plans |
| 3 | **FORGE** | gemini-2.0-flash | Implements code changes, commits with audit metadata |
| 4 | **SENTINEL** | gemini-2.0-pro | Reviews code quality, creates MRs (merged HERALD functionality) |
| 5 | **PHOENIX** | gemini-2.0-flash | Manages releases, notifies stakeholders |

> **Note**: HERALD agent functionality has been merged into SENTINEL.

## Key Features

âœ… **Automated SDLC Pipeline**: From requirements to release  
âœ… **Full Audit Trail**: Every agent execution tracked with state snapshots  
âœ… **Git Integration**: Commits include agent metadata for traceability  
âœ… **Repository Management**: Automated cloning, branching, and artifact storage  
âœ… **Real-time Updates**: WebSocket status notifications  
âœ… **Structured Logging**: Daily rotation with task correlation IDs

## Quick Start

### Prerequisites
- Python 3.11+
- Docker (for RabbitMQ)
- Google Gemini API Key

### Backend Setup

```bash
# Navigate to backend
cd backend

# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Create .env file
echo "GOOGLE_API_KEY=your_key_here" > .env
echo "STORAGE_PATH=./storage" >> .env

# Run database migrations
python -m app.db.migrate

# Start FastAPI server
uvicorn app.main:app --reload --port 8000
```

### Start RabbitMQ & Celery Worker

```bash
# Start RabbitMQ (Docker)
docker-compose up -d

# Start Celery worker (in separate terminal)
cd backend
celery -A app.celery_app worker --loglevel=info --concurrency=2
```

### Frontend Setup

**Current (Phase 2)**:
```bash
# Simply open in browser
start frontend/index.html  # Windows
open frontend/index.html   # Mac
```

**Phase 3 (React + Tailwind - In Progress)**:
```bash
cd frontend
npm install
npm run dev
```

### Access Points
- **Frontend**: http://localhost:3000 (or file:// for Phase 2)
- **Backend API**: http://localhost:8000
- **Swagger Docs**: http://localhost:8000/docs
- **RabbitMQ Admin**: http://localhost:15672 (guest/guest)

## Environment Configuration

Create `.env` file in `backend/`:

```env
# Google Gemini API Key (Required)
GOOGLE_API_KEY=your_gemini_api_key_here

# Storage Path
STORAGE_PATH=./storage

# Database
DATABASE_URL=sqlite:///./dev.db
# DATABASE_URL=postgresql://user:pass@localhost/sdlc_agents

# RabbitMQ
RABBITMQ_URL=amqp://guest:guest@localhost:5672//

# Worker limits
MAX_WORKERS=5
```

## Project Structure

```
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents/          # Agent implementations (SCRIBE, ARCHITECT, etc.)
â”‚   â”‚   â”œâ”€â”€ api/             # REST endpoints + WebSocket
â”‚   â”‚   â”œâ”€â”€ models/          # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ services/        # RepoService, ArtifactService, AuditService
â”‚   â”‚   â””â”€â”€ tasks/           # Celery task orchestration
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ agents.yaml      # Agent configurations
â”‚   â”œâ”€â”€ storage/             # Generated at runtime (gitignored)
â”‚   â”‚   â”œâ”€â”€ logs/            # Structured logs
â”‚   â”‚   â”œâ”€â”€ repos/           # Cloned repositories
â”‚   â”‚   â”œâ”€â”€ artifacts/       # Generated documents
â”‚   â”‚   â”œâ”€â”€ audit/           # Agent state snapshots
â”‚   â”‚   â””â”€â”€ patches/         # Code review diffs
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/                # React + Tailwind (Phase 3)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ agents.md
â”œâ”€â”€ docker-compose.yml       # RabbitMQ
â””â”€â”€ README.md
```

## Documentation

- [Architecture](docs/architecture.md) - System design, data flow, scalability
- [Agents](docs/agents.md) - Agent specifications, guardrails, audit trail

## Agent Audit Trail

Every agent execution is automatically tracked:

```bash
# Get all agent executions for a task
curl http://localhost:8000/api/audit/task/{task_id}

# Get full agent config by state ID
curl http://localhost:8000/api/audit/state/{state_id}

# Find agent state from Git commit (Forge only)
curl http://localhost:8000/api/audit/commit/{commit_hash}
```

**Example Git Commit with Metadata**:
```
[FORGE-AI] Implemented feature based on technical plan

Agent-State-ID: a3b8d1b6-0b3b-4b1a-9c1a-1a2b3c4d5e6f
Model: gemini-2.0-flash
Temperature: 0.2
Task: task_12345
Timestamp: 2026-02-10T01:05:00.123456
Provider: google

Generated by AI Agent Pipeline
```

## Running Tests

```bash
cd backend
pytest tests/ -v --cov=app
```

## Development Phases

- âœ… **Phase 1**: Foundation (FastAPI, Celery, RabbitMQ, basic frontend)
- âœ… **Phase 2A**: Infrastructure (Structured logging, Gemini integration)
- âœ… **Phase 2B**: Storage (Repository & artifact services)
- âœ… **Phase 2C**: Agents (SCRIBE, ARCHITECT, FORGE, SENTINEL)
- âœ… **Phase 2D**: Audit Trail (State snapshots, Git metadata)
- ðŸš§ **Phase 3**: React + Tailwind frontend migration
- ðŸ“‹ **Phase 4**: Connectors, Tools, MCP integration
- ðŸ“‹ **Phase 5**: Human-in-the-Loop workflows
- ðŸ“‹ **Phase 6**: PHOENIX full implementation, cleanup

## License

Internal use only.
